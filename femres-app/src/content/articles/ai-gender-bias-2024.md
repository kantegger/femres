---
title: "人工智能中的性别偏见：2024年的警钟"
author: "联合国妇女署 & UNESCO"
publishDate: 2024-03-08
description: "最新研究显示，AI系统中44%存在性别偏见，女性在技术领域的代表性不足正在加剧这一问题。从ChatGPT到图像生成器，AI正在强化有害的性别刻板印象。"
featuredImage: "/images/articles/ai-gender-bias-2024.jpg"
topics: ["数字女性主义", "AI伦理", "性别歧视", "技术批判", "职场平等"]
status: "published"
sourceUrl: "https://www.unwomen.org/en/articles/explainer/artificial-intelligence-and-gender-equality"
readingTime: 8
contentLanguage: "en"
---

## AI偏见的惊人证据

2024年国际妇女节前夕，UNESCO的一项研究揭示了大型语言模型（LLM）中令人担忧的趋势：不仅存在性别偏见，还有恐同和种族刻板印象。研究发现：

- 女性被描述为从事家务劳动的频率是男性的**4倍**
- 女性名字经常与"家庭"、"孩子"、"厨房"等词汇关联
- 男性名字则与"商业"、"执行官"、"薪水"和"职业"相关联

### 133个AI系统的分析

伯克利哈斯性别与领导力平等中心分析了不同行业的133个AI系统，发现：
- **44%** 的系统显示出性别偏见
- **25%** 同时表现出性别和种族偏见
- 提示词"律师"不成比例地生成看起来像年长白人男性的图像
- 提示词"护士"倾向于生成看起来像女性的图像

## 女性正在回避AI技术

纽约联邦储备银行2024年的调查显示了一个令人担忧的趋势：

**使用生成式AI的比例：**
- 男性：50%（过去12个月内）
- 女性：仅33%

在大多数研究中，采用AI工具的女性比例比男性少**10%到40%**。女性回避AI的主要原因是对使用这些工具的伦理性存在质疑。

### 职业影响

如果女性继续远离AI技术，可能会导致：
- 薪酬差距进一步扩大
- 就业机会减少
- 在技术驱动的未来经济中处于劣势

## AI如何强化刻板印象

### 训练数据的偏见

"人工智能反映了我们社会中存在的偏见，这些偏见体现在AI训练数据中。"——AI研究员Beyza Doğuç

由于大多数具有女性特征的AI主要由男性开发，它们反映了男性对女性的想法，这凸显了增加女性参与STEM教育和职业的必要性。

### 代表性危机

来自卢旺达参加编程营的学生Natacha Sangwa观察到："我注意到[AI]主要由男性开发，并在主要基于男性的数据集上进行训练。"

## 解决方案与行动

### 女性主义数据实践

- 分析权力如何运作并使用数据挑战不平等的权力结构
- 超越性别二元论
- 重视多种形式的知识
- 优先考虑本地和原住民知识

### 企业承诺

2024年2月，包括微软在内的8家全球科技公司认可了UNESCO的AI伦理建议，呼吁：
- 确保AI工具设计中的性别平等
- 为公司的性别平等计划提供专项资金
- 财政激励女性创业

### 政策建议

1. **增加女性在AI开发中的参与**
   - 设立配额和激励措施
   - 提供培训和教育机会
   - 创造包容性的工作环境

2. **审计现有系统**
   - 定期检查AI系统的偏见
   - 建立透明的报告机制
   - 实施纠正措施

3. **多样化数据集**
   - 确保训练数据的代表性
   - 包含多元化的声音和经验
   - 避免历史偏见的延续

## 紧迫性

随着AI技术的快速发展和广泛应用，解决性别偏见问题变得更加紧迫。如果不采取行动，这些系统将继续强化和放大现有的不平等，影响数十亿人的生活机会。

正如联合国妇女署所强调的："我们必须确保AI的发展不会倒退我们在性别平等方面取得的进展，而是成为推动平等的工具。"

## 行动呼吁

每个人都可以为创造更公平的AI未来做出贡献：
- **教育自己**了解AI偏见
- **支持**致力于道德AI的组织
- **倡导**工作场所和社区的变革
- **参与**技术发展，确保多元化的声音被听到

未来的AI必须是女性主义的AI——包容、公平、代表所有人的AI。