---
title: "人工智能中的性别偏见与数字女性主义：通过数字素养赋权女性"
originalTitle: "Gender Bias in Artificial Intelligence and Digital Feminism: Empowering Women Through Digital Literacy"
author: "Premier Science Research Team"
publishDate: 2025-01-25
journal: "Premier Journal of AI & Society"
description: "本文调查了人工智能系统中性别偏见与数字素养在科技领域赋权女性潜力之间的相互作用。通过综合2010-2024年的研究，探讨性别偏见如何在AI中体现及数字素养倡议的有效性。"
topics: ["数字女性主义", "算法偏见", "人工智能", "性别平等", "科技批判"]
status: "published"
contentLanguage: "en"
doi: "10.1108/PJAI-24-524"
readingTime: 28
keywords: ["AI偏见", "数字女性主义", "算法歧视", "数字素养", "科技批判"]
citationCount: 0
paperType: research
sourceUrl: "https://premierscience.com/pjai-24-524/"
abstract: "系统性性别偏见嵌入在招聘、医疗保健和金融服务等不同领域的AI应用中。全球仅有22%的AI工作者是女性，44%的AI系统显示性别偏见，25%同时表现出性别和种族偏见。"
---

## 引言：算法时代的性别正义

人工智能已经成为日常生活不可或缺的一部分，涵盖了全球社会的各个方面。鉴于AI在社会中的渗透，以及对AI偏见的担忧，人文和社会科学领域对AI的批判性学术研究已经获得了关注。特别有力的AI批判包括受美国黑人女性主义启发的学术著作。

2025年1月发表的一项叙述性综述调查了人工智能系统中的性别偏见与数字素养在科技领域赋权女性潜力之间的相互作用。该研究通过综合2010至2024年的研究，检验了性别偏见如何在AI中体现、其对女性参与科技的影响，以及数字素养倡议在解决这些差异方面的有效性。

## AI性别偏见的表现形式

### 数据中的历史偏见

AI系统的核心问题之一是它们依赖历史数据进行训练。这些数据往往反映并延续了现有的社会偏见。当AI系统在历史上带有偏见的数据上进行训练时，它们可能保持甚至加剧性别差异。

伯克利哈斯性别与领导力平等中心的一项研究分析了不同行业的133个AI系统，发现约44%显示性别偏见，25%同时表现出性别和种族偏见。这些偏见不是偶然的故障，而是系统如何设计和训练的结构性结果。

### 语言模型中的性别刻板印象

大型语言模型的分析揭示了令人不安的模式。女性被描述为从事家务劳动的频率远高于男性——某个模型中是四倍——并且经常与"家庭"、"家人"和"孩子"等词联系在一起，而男性名字则与"商业"、"高管"、"薪水"和"职业"相关联。

这些关联不仅仅是无害的统计模式。它们影响着AI系统如何做出影响真人的决策——从招聘推荐到贷款批准，从医疗诊断到法律判决。

### 行业特定的偏见

**招聘和人力资源**：基于AI的招聘工具对女性的帮助较小。亚马逊著名地放弃了一个显示对女性有偏见的AI招聘工具，对包含"女性"一词的简历（如"女子国际象棋俱乐部队长"）进行降级。

**语音识别**：语音识别系统通常在女性声音上表现不佳。研究表明，领先的语音识别系统对男性声音的准确率比女性声音高13%。

**医疗保健**：AI诊断工具可能基于有偏见的训练数据对女性健康状况进行误诊或诊断不足，历史上医学研究中女性代表性不足。

**金融服务**：信用评分算法和贷款批准系统可能系统性地不利于女性，特别是那些有照顾责任或非线性职业道路的女性。

## 结构性原因分析

### AI劳动力中的代表性不足

全球仅有22%的AI工作者是女性。这种代表性不足产生了连锁效应：

**设计盲点**：男性主导的团队可能没有意识到或优先考虑影响女性的问题。

**测试不足**：产品可能没有充分测试各种用户群体，导致对某些人口统计数据的性能较差。

**文化偏见**：工作场所文化可能无意中排斥或边缘化女性，延续了代表性不足的循环。

### 算法设计选择

性别偏见源于多种因素，包括AI开发团队中女性代表性不足、带偏见的训练数据集和算法设计选择。这些不是中性的技术决策，而是反映并强化社会价值观和优先事项的选择。

例如，优化参与度或点击率的算法可能无意中推广强化性别刻板印象的内容，因为这些内容由于确认偏见往往会产生更高的参与度。

## 数字女性主义的兴起

### 作为抵抗工具的技术

尽管存在这些挑战，数字技术也成为了女性主义抵抗的有力工具。数字女性主义包括使用数字平台和工具来推进性别平等和挑战父权结构。

计算机科学家、作家和活动家Joy Buolamwini和Timnit Gebru的行动主义，她们明确推广黑人女性主义视角，同时提高人们对跨性别技术用户经历的认识，已经影响了围绕面部识别技术的监管努力。Buolamwini通过在国会就面部识别技术中的种族和性别偏见作证，参与了美国国家的核心机构。

### 女性视角与行动主义

从数字对话中女性用户互动收集的实证数据突出显示，最突出的兴趣主题是AI技术的未来和女性在保证性别平衡系统中的积极作用。算法偏见影响女性用户对算法结果中不公正和不平等的响应行为。她们分享感兴趣的主题，并与性别或种族赋权协会相关的档案进行建设性对话。受到刻板印象和偏见挑战的女性可能会资助创业解决方案，为变革创造机会。

### 社交媒体作为组织空间

2025年Fraile-Rojas及同事的一项研究探讨了使用自然语言处理（NLP）技术和机器学习（ML）模型来发现女性社交媒体对话中应用于人工智能（AI）技术的性别不平等的潜在概念。

社交媒体平台已成为：

**意识提升**：分享关于AI偏见的信息和经验。

**集体行动**：组织活动和倡议以挑战技术中的性别歧视。

**知识生产**：众包研究和记录偏见实例。

**支持网络**：为科技领域的女性建立社区和指导。

## 数字素养作为干预手段

### 转型工具

数字素养计划成为一种有前景的干预措施，培养对AI偏见的批判性认识，鼓励女性从事AI职业，并促进女性主导的AI项目的增长。尽管AI中的性别偏见带来了重大挑战，但这项综述强调数字素养是实现AI开发和应用中性别平等的转型工具。

数字素养不仅仅是技术技能；它包括：

**批判性AI素养**：理解AI系统如何工作，它们的局限性，以及偏见如何嵌入其中。

**数据素养**：理解数据如何收集、处理和用于决策。

**算法意识**：认识到算法何时做出影响人生活的决策。

**数字权利知识**：理解隐私、同意和数字环境中的权利。

### 成功的数字素养计划

有效的数字素养计划具有几个关键特征：

**交叉性方法**：认识到性别与种族、阶级、年龄和其他身份因素相交。

**实践学习**：提供AI工具和技术的实际操作经验。

**批判性视角**：鼓励质疑和挑战现有系统，而不仅仅是适应它们。

**社区建设**：创建支持网络和指导机会。

**政策参与**：将技术技能与倡导和政策变革联系起来。

## 制度和政策回应

### 监管框架

世界各地的政府和国际组织正在制定解决AI偏见的框架：

**欧盟AI法案**：包括防止歧视的具体条款，并要求高风险AI系统的透明度。

**UNESCO关于AI伦理的建议**：强调性别平等和在AI开发各个阶段包容女性。

**国家倡议**：各国正在制定自己的AI伦理指南和法规，越来越多地纳入性别考虑。

### 企业责任

科技公司面临越来越大的压力来解决其产品中的性别偏见：

**多样性倡议**：增加AI团队中女性代表性的计划。

**偏见审计**：定期测试和评估AI系统的歧视性结果。

**透明度报告**：公开劳动力多样性和AI系统性能的数据。

**道德委员会**：建立监督小组来审查AI开发和部署。

## 全球视角与文化考虑

### 南半球的挑战

发展中国家的女性在AI和数字技术方面面临着复合挑战：

**基础设施障碍**：有限的互联网接入和数字设备。

**教育差距**：STEM教育机会较少。

**文化规范**：限制女性参与技术的社会期望。

**经济约束**：数字工具和教育的成本。

### 本土和非殖民化方法

越来越多的人认识到需要多样化的技术开发方法，这些方法不仅仅复制西方模式：

**本土AI**：整合本土知识系统和价值观到AI开发中。

**社区主导的解决方案**：支持解决特定性别不平等的地方倡议。

**语言多样性**：开发适用于非英语语言和文化的AI系统。

## 未来方向与建议

### 包容性AI设计

该研究强调了包容性AI设计、性别响应教育政策和持续研究努力以减轻偏见和促进公平的重要性。

包容性设计原则包括：

**多样化团队**：确保AI开发团队中的性别、种族和背景多样性。

**参与式设计**：让不同的用户群体参与设计过程。

**交叉性测试**：评估AI系统对不同交叉身份的影响。

**持续监测**：实施系统以检测和纠正部署后出现的偏见。

### 教育改革

需要对教育系统进行根本性改变：

**早期干预**：从小学开始引入计算思维和AI素养。

**课程改革**：将性别研究和伦理整合到技术教育中。

**教师培训**：为教育工作者配备解决课堂中性别偏见的工具。

**榜样和指导**：突出成功的女性技术专家并创建指导计划。

### 研究议程

未来的研究应该关注：

**长期影响**：跟踪数字素养干预的长期结果。

**交叉性分析**：检查性别如何与其他身份因素相交影响AI体验。

**全球比较研究**：理解不同文化背景下的性别偏见如何表现。

**技术创新**：开发新工具和方法来检测和减轻偏见。

## 结论：走向性别公正的AI未来

2025年关于数字女性主义、算法偏见和AI性别歧视的研究表明，虽然AI中的性别偏见仍然是一个重大挑战，但对数字女性主义方法的学术关注日益增加，强调数字素养、包容性设计和交叉性框架是解决算法歧视的关键策略。

道德AI涉及在解决性别、种族、民族、社会经济地位和其他决定因素的问题时采取交叉性方法，除了采用以透明度、问责制和人类尊严为前提的基于人权的AI治理方法。不同的利益相关者，包括商业和企业实体、科技公司、学术界、联合国实体、民间社会组织、媒体和其他相关行为者应该联合起来，探索共同的解决方案。

正如研究所示，虽然挑战是真实和重大的，但通过协调努力、批判性意识和对公平的承诺实现改变的潜力同样强大。未来不是预先确定的；通过认识和解决AI中的性别偏见，我们可以努力创造一个技术服务于所有人的更公正的数字未来，无论性别如何。

通往性别公正AI的道路需要技术创新、政策干预、教育改革和文化变革的结合。这不仅仅是一个技术挑战，而是一个社会挑战，需要所有利益相关者的集体行动。只有通过这种全面的方法，我们才能确保AI革命不会延续过去的不平等，而是有助于创造一个更加平等的未来。